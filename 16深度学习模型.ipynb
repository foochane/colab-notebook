{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import gensim\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "if not os.path.exists('output/data_clean_split.txt'):\n",
    "  !wget -P ./output https://raw.githubusercontent.com/foochane/text-classification/master/output/data_clean_split.txt\n",
    "\n",
    "# 读取数据\n",
    "labels = []\n",
    "text = []\n",
    "with codecs.open('output/data_clean_split.txt','r',encoding='utf-8') as f:\n",
    "    document_split = f.readlines()\n",
    "    for document in document_split:\n",
    "        temp = document.split('\\t')\n",
    "        labels.append(temp[0])\n",
    "        text.append(temp[1].strip())  \n",
    "\n",
    "# 标签转换为数字\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "\n",
    "\n",
    "# 切分数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(text, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "\n",
    "# 将每个句子切分成单个词\n",
    "text_s2w= [s.split() for s in text]\n",
    "\n",
    "#训练word2vec模型\n",
    "model = gensim.models.Word2Vec(text_s2w,\n",
    "                               min_count=5,\n",
    "                               workers=6,\n",
    "                               window =8,\n",
    "                               size=100)\n",
    "\n",
    "#该函数会将语句转化为一个标准化的向量（Normalized Vector）\n",
    "def sent2vec(s):\n",
    "    \"\"\"\n",
    "    将每个句子转换会一个100的向量\n",
    "    \"\"\"\n",
    "    words = s.split()\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            #M.append(embeddings_index[w])\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)  # shape=(x,100),x是句子中词的个数，100是每个词向量的维数\n",
    "    v = M.sum(axis=0) # 维度是100，对M中的x个数求和，得到每一维度的总和\n",
    "    if type(v) != np.ndarray: \n",
    "        return np.zeros(100)\n",
    "    \n",
    "    return v / np.sqrt((v ** 2).sum()) # 正则化，最后每个句子都变为一100维的向量\n",
    "\n",
    "# 对训练集和验证集使用上述函数，进行文本向量化处理\n",
    "text_s2v = [sent2vec(s) for s in tqdm(text)]\n",
    "\n",
    "# 转换成numpy array数组\n",
    "text_s2v = np.array(text_s2v)\n",
    "\n",
    "#构建词建词嵌入字典\n",
    "embeddings_index = dict(zip(model.wv.index2word, model.wv.vectors))\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "# 切分数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train_w2v, x_valid_w2v, y_train, y_valid = train_test_split(text_s2v, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在使用神经网络前，对数据进行缩放\n",
    "scl = preprocessing.StandardScaler()\n",
    "x_train_w2v_scl = scl.fit_transform(x_train_w2v)\n",
    "x_valid_w2v_scl = scl.transform(x_valid_w2v)\n",
    "\n",
    "# 对标签进行binarize处理\n",
    "y_train_enc = np_utils.to_categorical(y_train)\n",
    "y_valid_enc = np_utils.to_categorical(y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_enc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid_w2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全连接网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建1个3层的序列神经网络（Sequential Neural Net）\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(19))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# 模型编译\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(x_train_w2v_scl, \n",
    "          y=y_train_enc, \n",
    "          batch_size=64, \n",
    "          epochs=5, \n",
    "          verbose=1, \n",
    "          validation_data=(x_valid_w2v_scl, y_valid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 keras tokenizer\n",
    "token = keras.preprocessing.text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(x_train) + list(x_valid))\n",
    "x_train_seq = token.texts_to_sequences(x_train)\n",
    "x_valid_seq = token.texts_to_sequences(x_valid)\n",
    "\n",
    "#对文本序列进行zero填充\n",
    "x_train_pad = sequence.pad_sequences(x_train_seq, maxlen=max_len)\n",
    "x_valid_pad = sequence.pad_sequences(x_valid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基于已有的数据集中的词汇创建一个词嵌入矩阵（Embedding Matrix）\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于前面训练的Word2vec词向量，使用1个两层的LSTM模型\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     100,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(19))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在模型拟合时，使用early stopping这个回调函数（Callback Function）\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(x_train_pad, \n",
    "          y=y_train_enc, \n",
    "          batch_size=512, \n",
    "          epochs=100, \n",
    "          verbose=1, \n",
    "          validation_data=(x_valid_pad, y_valid_enc), \n",
    "          callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用双向长短时记忆(Bi-Directional LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于前面训练的Word2vec词向量，构建1个2层的Bidirectional LSTM \n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     100,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(19))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在模型拟合时，使用early stopping这个回调函数（Callback Function）\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(x_train_pad, \n",
    "          y=y_train_enc, \n",
    "          batch_size=512, \n",
    "          epochs=100, \n",
    "          verbose=1, \n",
    "          validation_data=(x_valid_pad, y_valid_enc), \n",
    "          callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于前面训练的Word2vec词向量，构建1个2层的GRU模型\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     100,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(100, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(19))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在模型拟合时，使用early stopping这个回调函数（Callback Function）\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(x_train_pad, \n",
    "          y=y_train_enc, \n",
    "          batch_size=512, \n",
    "          epochs=100, \n",
    "          verbose=1, \n",
    "          validation_data=(x_valid_pad, y_valid_enc), \n",
    "          callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
